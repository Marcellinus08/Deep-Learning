{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8LieN98vwsDH/GnnU9PBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcellinus08/Deep-Learning/blob/main/Loading_and_Preprocessing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZZ4AjkfiukqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library"
      ],
      "metadata": {
        "id": "oGXU889fumef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gzkNpqZLuVUX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import os\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep"
      ],
      "metadata": {
        "id": "VOC0zsyyuq1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
      ],
      "metadata": {
        "id": "p8DI6tyrurXE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisasi data"
      ],
      "metadata": {
        "id": "NkkkYOxduuIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_\n",
        ""
      ],
      "metadata": {
        "id": "sAYT0z_luu-k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, f\"my_{name_prefix}_{{:02d}}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"w\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([str(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths"
      ],
      "metadata": {
        "id": "JwQ_TKS9uyGd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)"
      ],
      "metadata": {
        "id": "UgFlq3Iquy-f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to File"
      ],
      "metadata": {
        "id": "QtTfmKpDu1ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "metadata": {
        "id": "p7pDBRmYu179"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelining"
      ],
      "metadata": {
        "id": "4_7lJsu7u4Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_csv_line(line):\n",
        "    defs = [0.] * len(header_cols) # Default untuk setiap kolom adalah float\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filepaths, n_readers=5, n_parse_threads=5,\n",
        "                       n_shuffle=10000, batch_size=32):\n",
        "    # Membuat dataset dari path file\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    # Membaca dari beberapa file secara paralel\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers)\n",
        "    # Parsing baris CSV secara paralel\n",
        "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
        "    # Shuffle data\n",
        "    dataset = dataset.shuffle(n_shuffle)\n",
        "    # Batch data dan prefetch\n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "B1u-5MkOu4Am"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat dataset untuk training"
      ],
      "metadata": {
        "id": "7dkbFOMju84T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, batch_size=32)\n",
        "valid_set = csv_reader_dataset(valid_filepaths, batch_size=32)\n",
        "test_set = csv_reader_dataset(test_filepaths, batch_size=32)\n",
        ""
      ],
      "metadata": {
        "id": "u2x_C04Pu9Ns"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Model"
      ],
      "metadata": {
        "id": "O1KRdACAu_zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Standardization(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mean = tf.constant(X_mean, dtype=tf.float32)\n",
        "        self.std = tf.constant(X_std, dtype=tf.float32)\n",
        "    def call(self, inputs):\n",
        "        return (inputs - self.mean) / self.std\n",
        ""
      ],
      "metadata": {
        "id": "iCs9KPQQvArj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = housing.data.shape[1]\n",
        "model = keras.models.Sequential([\n",
        "    Standardization(input_shape=[n_features]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACraJFhGvCgh",
        "outputId": "c67a56cb-7cf5-4d8e-d77a-b9d6baccfef3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1826545342>:3: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "72BeyEkzvFH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"RootMeanSquaredError\"])\n",
        "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tJkc2awvFgX",
        "outputId": "b8353c31-d625-42b8-a354-225e1653970c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "    351/Unknown \u001b[1m7s\u001b[0m 6ms/step - RootMeanSquaredError: 1.6658 - loss: 2.8632"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - RootMeanSquaredError: 1.6523 - loss: 2.8202 - val_RootMeanSquaredError: 1.0932 - val_loss: 1.1951\n",
            "Epoch 2/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - RootMeanSquaredError: 0.7687 - loss: 0.5912 - val_RootMeanSquaredError: 0.6986 - val_loss: 0.4881\n",
            "Epoch 3/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - RootMeanSquaredError: 0.6862 - loss: 0.4712 - val_RootMeanSquaredError: 0.6428 - val_loss: 0.4131\n",
            "Epoch 4/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6596 - loss: 0.4352 - val_RootMeanSquaredError: 0.6923 - val_loss: 0.4792\n",
            "Epoch 5/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6340 - loss: 0.4020 - val_RootMeanSquaredError: 0.7661 - val_loss: 0.5869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi"
      ],
      "metadata": {
        "id": "IxH6NmYEvKD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluasi pada test set:\")\n",
        "mse_test, rmse_test = model.evaluate(test_set)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rn8ds0dvKhc",
        "outputId": "983812e3-2414-47f0-b044-5d0b46f1453b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluasi pada test set:\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6316 - loss: 0.3996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMembuat prediksi pada data baru:\")\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow2JQkdHvN2M",
        "outputId": "67e81d84-afb0-4c97-9305-426a6fa7126f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Membuat prediksi pada data baru:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediksi:\", y_pred.flatten())\n",
        "print(\"Label Sebenarnya:\", y_test[:3].flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEzoGd7xvQqk",
        "outputId": "0fbc25a6-50d4-4e8d-ab62-0c69983d67bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediksi: [0.42399067 1.8138095  3.5486183 ]\n",
            "Label Sebenarnya: [0.477   0.458   5.00001]\n"
          ]
        }
      ]
    }
  ]
}
